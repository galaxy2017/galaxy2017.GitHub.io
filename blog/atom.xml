<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://blog.luomoe.com/blog</id>
    <title>胖螺 Blog</title>
    <updated>2023-07-31T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://blog.luomoe.com/blog"/>
    <subtitle>胖螺 Blog</subtitle>
    <icon>https://blog.luomoe.com/https://img.up.cdn.nahida.cn/2020/03/cropped-logo2-1.png</icon>
    <entry>
        <title type="html"><![CDATA[边缘计算相关论文笔记（2023年7月）]]></title>
        <id>IoV202307</id>
        <link href="https://blog.luomoe.com/blog/IoV202307"/>
        <updated>2023-07-31T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[001  Energy-Efficient Joint Task Offloading and Resource Allocation in OFDMA-Based Collaborative Edge Computing]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="001--energy-efficient-joint-task-offloading-and-resource-allocation-in-ofdma-based-collaborative-edge-computing">001  Energy-Efficient Joint Task Offloading and Resource Allocation in OFDMA-Based Collaborative Edge Computing<a class="hash-link" href="#001--energy-efficient-joint-task-offloading-and-resource-allocation-in-ofdma-based-collaborative-edge-computing" title="标题的直接链接">​</a></h2><p>This article is a study on OFDMA-based collaborative mobile edge computing (C-MEC). The article first introduces the background and advantages of C-MEC, and then presents a joint optimization problem for task offloading, collaborative decision making, and resource allocation. The article models a mixed integer nonlinear programming (MINLP) problem with the objective of minimizing the total energy consumption of all mobile users while satisfying task delay constraints. Since this problem is NP-hard, the article proposes a two-layer framework of alternating methods to solve it. In the first layer, the article utilizes an ant colony system (ACS)-based heuristic algorithm to optimize task offloading decisions; in the second layer, the article utilizes a deep reinforcement learning algorithm based on deep Q-networks (DQN) to optimize resource allocation. The article verifies the excellent performance of the proposed algorithm in terms of energy efficiency and task completion rate through simulation experiments. The experimental results show that the proposed algorithm can effectively reduce the energy consumption of mobile users and ensure the task completion within the specified time. In addition, the convergence and robustness of the algorithm are analyzed in the paper.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="002--the-case-for-fpga-based-edge-computing">002  The Case for FPGA-Based Edge Computing<a class="hash-link" href="#002--the-case-for-fpga-based-edge-computing" title="标题的直接链接">​</a></h2><p>This article focuses on an FPGA-based edge computing model that takes advantage of the customizability of FPGAs and the low latency of edge computing to accelerate the response time and save energy of mobile interactive applications. The article selects three typical computer vision applications as case studies, namely, handwritten digit recognition, object recognition, and face detection. The article experimentally compares the performance of four schemes: FPGA edge offload, CPU edge offload, CPU cloud offload, and mobile local processing, and the results show that FPGA edge offload outperforms the other schemes in terms of response time, execution time, and energy consumption. The article also explores data parallel processing methods between mobile and edge nodes to further reduce the response time of batch requests. The article concludes with a discussion of the advantages, limitations, and future research directions of the FPGA edge computing model.</p><h1>第二周</h1><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="003--joint-task-offloading-and-cache-placement-for-energy-efficient-mobile-edge-computing-systems">003  Joint Task Offloading and Cache Placement for Energy-Efficient Mobile Edge Computing Systems<a class="hash-link" href="#003--joint-task-offloading-and-cache-placement-for-energy-efficient-mobile-edge-computing-systems" title="标题的直接链接">​</a></h2><p>This article is about joint task cache placement and offloading design for cache-enabled multi-user Mobile Edge Computing (MEC) systems. The goal of the article is to minimize the total system-weighted energy consumption in the task caching and task arrival/execution phases, taking into account the constraints of cache capacity, task causality, and task completion deadline. The article first solves the optimal offline solution of the problem using the branch-and-bound (BnB) method, and then proposes two low-complexity schemes based on task popularity and convex relaxation. The article demonstrates the advantages of the proposed schemes over existing benchmark schemes through numerical results.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="004-energy-efficient-computation-offloading-in-mobile-edge-computing-systems-with-uncertainties">004 Energy-Efficient Computation Offloading in Mobile Edge Computing Systems With Uncertainties<a class="hash-link" href="#004-energy-efficient-computation-offloading-in-mobile-edge-computing-systems-with-uncertainties" title="标题的直接链接">​</a></h2><p>This article is about the problem of energy-efficient computational offloading in mobile edge computing systems. The article proposes a new approach to this problem that relaxes the strong assumptions on radio channel and network queue sizes made in existing research and takes into account the uncertainty inherent in the network. The article uses extreme value theory to limit the probability of occurrence of uncertain events and develops a column generation-based ε-bounded approximation algorithm to solve the posed problem. The algorithm is effective in finding a feasible solution that is less than (1 + ε) times the optimal solution. The article also implements the proposed scheme on an Android smartphone and conducts extensive experiments using real-world applications. The experimental results confirm that the energy consumption of the client device can be reduced by taking into account the inherent uncertainty in the computational offloading process. The proposed computational offloading scheme also significantly outperforms other schemes in terms of energy savings.</p><h1>第三周</h1><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="005-joint-power-control-and-computation-offloading-for-mobile-edge-networks">005 Joint power control and computation offloading for mobile edge networks<a class="hash-link" href="#005-joint-power-control-and-computation-offloading-for-mobile-edge-networks" title="标题的直接链接">​</a></h2><p>This article investigates how to minimize the energy consumption of mobile devices by offloading computationally intensive tasks to MEC servers, taking into account co-channel interference and task latency requirements. The article presents an analytical model to decouple power control and computational resource allocation and shows that the joint optimization problem is invex and can be solved by a CCP-based algorithm. The article also proves that the joint power and CPU cycle allocation problem is a type I invex problem, which guarantees that each KKT stabilization point of the problem is a global minimum. The article also provides an offloading decision criterion for optimal energy efficiency computation based on the partial derivatives of the total energy consumption of the mobile device.The article models the communication channel as block fading and the computational task as a tuple of input data size, required CPU cycles, and maximum latency. The article defines the transmission power, rate, latency, and energy consumption of each offloaded mobile device, as well as the local execution power, latency, and energy consumption. The article also introduces offloading decision variables to indicate whether a mobile device chooses to offload its task or not.The article proves that the total transmission energy consumption function is a concave function monotonically increasing with respect to each power configuration component. The article also derives a set of linear equations to represent the relationship between transmission power and computational resources, where the coefficient matrix is an inverse positive M matrix that depends on the CPU cycle allocation.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="006-joint-offloading-and-resource-allocation-using-deep-reinforcement-learning-in-mobile-edge-computing">006 Joint Offloading and Resource Allocation Using Deep Reinforcement Learning in Mobile Edge Computing<a class="hash-link" href="#006-joint-offloading-and-resource-allocation-using-deep-reinforcement-learning-in-mobile-edge-computing" title="标题的直接链接">​</a></h2><p>This article investigates the problem of partial task offloading and resource allocation in Mobile Edge Computing (MEC). The article proposes a Deep Reinforcement Learning (DRL)-based Energy Efficiency Algorithm (EEDRL) that decomposes the original non-convex optimization problem into two sub-problems, i.e., offloading ratio selection and resource allocation.The EEDRL employs an actor-critic network architecture, where the actor network learns the optimal mapping from the time-varying wireless channel to offloading ratios, and the critic network utilizes an advanced convex optimization algorithm to solve the the resource allocation subproblem.EEDRL devises an annealed Gaussian noise addition method for exploring more satisfactory offloading ratios in actor networks and explores different exploration strategies and verifies the generalization of the method. Numerical experiments are conducted to compare the method with various existing offloading schemes, and the results show that EEDRL is able to save up to 57.6% of energy consumption relative to binary offloading and achieves significant computation time speedups relative to the SQP algorithm. It is also shown that jointly optimizing the energy consumption of SMDs and MEC servers by choosing appropriate weighting factors for the MEC servers can reduce up to half of the total energy consumption, relative to a greedy strategy that only considers the energy reduction of SMDs.</p><h1>第四周</h1><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="007--energy-efficient-resource-management-in-uav-assisted-mobile-edge-computing">007  Energy-Efficient Resource Management in UAV-Assisted Mobile Edge Computing<a class="hash-link" href="#007--energy-efficient-resource-management-in-uav-assisted-mobile-edge-computing" title="标题的直接链接">​</a></h2><p>This paper investigates the energy efficiency optimization problem in UAV-assisted mobile edge computing systems with the goal of minimizing the energy consumption of mobile devices and UAVs. The paper considers factors such as UAV trajectory optimization, communication and computational resource allocation, and task offloading, and presents a non-convex optimization problem. To solve this problem, this paper introduces an algorithm based on block-by-block upper bound minimization (BSUM), which successively minimizes a tight upper bound of the objective function and updates the variables step by step. In this paper, we demonstrate the effectiveness of the proposed algorithm through numerical simulation results, which can significantly reduce the total energy consumption of the network compared to other benchmark algorithms.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="008--energy-efficient-task-offloading-and-resource-allocation-via-deep-reinforcement-learning-for-augmented-reality-in-mobile-edge-networks">008  Energy-Efficient Task Offloading and Resource Allocation via Deep Reinforcement Learning for Augmented Reality in Mobile Edge Networks<a class="hash-link" href="#008--energy-efficient-task-offloading-and-resource-allocation-via-deep-reinforcement-learning-for-augmented-reality-in-mobile-edge-networks" title="标题的直接链接">​</a></h2><p>This paper investigates the use of deep reinforcement learning in mobile edge networks for energy efficient task offloading and resource allocation optimization for augmented reality applications. The study builds a more specific and detailed model of an augmented reality application by dividing an application into five subtasks and considering the dependencies and latency requirements between the subtasks. In order to solve the hybrid problem of multi-user competition and cooperation and simultaneously satisfy the energy minimization and quality of service guarantee for each user, a multi-intelligent deep deterministic policy gradient (MADDPG) algorithm is proposed. The effectiveness and superiority of the proposed algorithm in single-edge server and multi-edge server systems are verified through simulation experiments.</p>]]></content>
        <author>
            <name>Pangluo</name>
            <uri>https://luomoe.com</uri>
        </author>
        <category label="Edge Computing" term="Edge Computing"/>
        <category label="Offloading" term="Offloading"/>
        <category label="Vehicles" term="Vehicles"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[解决NVIDIA 30/40系列显卡与tensorflow 1.15 不兼容的问题]]></title>
        <id>Technology20230729</id>
        <link href="https://blog.luomoe.com/blog/Technology20230729"/>
        <updated>2023-07-29T07:52:16.000Z</updated>
        <summary type="html"><![CDATA[问题起因]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="问题起因">问题起因<a class="hash-link" href="#问题起因" title="标题的直接链接">​</a></h2><p>因为显卡与显卡驱动、CUDA版本以及cuDNN版本有着非常严格的依赖关系。</p><blockquote><p>CUDA是NVIDIA推出的用于自家GPU的并行计算框架，也就是说CUDA只能在NVIDIA的GPU上运行，而且只有当要解决的计算问题是可以大量并行计算的时候才能发挥CUDA的作用。cuDNN是NVIDIA打造的针对深度神经网络的加速库，是一个用于深层神经网络的GPU加速库。如果你要用GPU训练模型，cuDNN不是必须的，但是一般会采用这个加速库。</p></blockquote><p>同时坊间传言，30系列显卡只支持CUDA 11.X 及往后的版本。40系列显卡只支持CUDA 11.3及往后的版本笔者在实践中，视乎也验证了上述的传言：</p><p><strong>30系列显卡：</strong></p><p><img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729093126683.png!blog.luomoe.com.20230729" alt="image-20230729093126683" class="img_ev3q"></p><p><strong>40系列显卡：</strong></p><p><img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729092952955.png!blog.luomoe.com.20230729" alt="image-20230729092952955" class="img_ev3q"></p><p>同时tensorflow对于依赖库也会有非常严格的要求。下标为经过测试的构建配置（GPU），来源官网：<a href="https://www.tensorflow.org/install/source?hl=zh-cn#tested_build_configurations" target="_blank" rel="noopener noreferrer">https://www.tensorflow.org/install/source?hl=zh-cn#tested_build_configurations</a></p><p><img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729093339504.png!blog.luomoe.com.20230729" alt="image-20230729093339504" class="img_ev3q"></p><p>如果盲目直接上手，一般都会直接部署tensorflow 1.15。一般错误为cuDNN以及CUDA版本不匹配（上图为30系列，下图为40系列显卡）：</p><p><img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729093752269.png!blog.luomoe.com.20230729" alt="image-20230729093752269" class="img_ev3q"></p><p><img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729093731526.png!blog.luomoe.com.20230729" alt="image-20230729093731526" class="img_ev3q"></p><p>综上所述，tensorflow的版本的对于cuDNN以及CUDA版本有着及其严格的要求。但是如果为了能够使用新购置的30/40系列显卡，就一定需要升级tensorflow版本并重写代码吗？并不一定。</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="解决方案">解决方案<a class="hash-link" href="#解决方案" title="标题的直接链接">​</a></h2><p>其实官方早已经提出了解决方案，也就是nvidia-tensorflow。</p><p>GitHub地址为：<a href="https://github.com/NVIDIA/tensorflow" target="_blank" rel="noopener noreferrer">https://github.com/NVIDIA/tensorflow</a></p><p>官方网站为：<a href="https://developer.nvidia.com/deep-learning-frameworks" target="_blank" rel="noopener noreferrer">https://developer.nvidia.com/deep-learning-frameworks</a></p><blockquote><p>NVIDIA has created this project to support newer hardware and improved libraries to NVIDIA GPU users who are using TensorFlow 1.x. With release of TensorFlow 2.0, Google announced that new major releases will not be provided on the TF 1.x branch after the release of TF 1.15 on October 14 2019. NVIDIA is working with Google and the community to improve TensorFlow 2.x by adding support for new hardware and libraries. However, a significant number of NVIDIA GPU users are still using TensorFlow 1.x in their software ecosystem. This release will maintain API compatibility with upstream TensorFlow 1.15 release. This project will be henceforth referred to as nvidia-tensorflow.</p><p>Link to Tensorflow <a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener noreferrer">README</a></p></blockquote><p>介绍：随着 TensorFlow 2.0 的发布，谷歌宣布 2019 年 10 月 14 日 TF 1.15 发布后，TF 1.x 分支将不再提供新的主要版本。英伟达正在与谷歌和社区合作，通过增加对新硬件和库的支持来改进 TensorFlow 2.x。然而，大量英伟达™（NVIDIA®）GPU 用户仍在其软件生态系统中使用 TensorFlow 1.x。该版本将保持与上游 TensorFlow 1.15 版本的 API 兼容。此后，该项目将被称为 nvidia-tensorflow。</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="安装步骤">安装步骤<a class="hash-link" href="#安装步骤" title="标题的直接链接">​</a></h2><ol><li>明确自己的安装的版本。可以直接打开该项目的GitHub地址。之后<strong>在左上角可以查看历史分支</strong>，查看有没有自己的所需的版本。</li></ol><p><img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729094821134.png!blog.luomoe.com.20230729" alt="image-20230729094821134" class="img_ev3q"></p><p><img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729094849381.png!blog.luomoe.com.20230729" alt="image-20230729094849381" class="img_ev3q"></p><p>​		主要是查看各个历史版本的所需要的安装系统的环境。查看readme自述文件中关于安装环境的要求，如有自己所需要的TF，则按readme自述文件部署安装环境：</p><p><img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729095024664.png!blog.luomoe.com.20230729" alt="image-20230729095024664" class="img_ev3q"></p><p><img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729095039532.png!blog.luomoe.com.20230729" alt="image-20230729095039532" class="img_ev3q"></p><ol start="2"><li><p>在准备好安装环境之后（推荐使用Anaconda，Miniconda安装部署所需要的环境，本文所使用的环境为Miniconda，具体操作与Anaconda一致）后输入以下命令行：</p><p>（1）检查pip版本：		</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip --version</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>​		如果不满足pip的需求，则需要更新pip版本：				</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> --upgrade pip</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>（2）NVIDIA wheels 不在 PyPI.org 上托管。要为 Tensorflow 安装英伟达™（NVIDIA®）wheels，请安装英伟达™（NVIDIA®）wheels索引：			</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> --user nvidia-pyindex</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>（3）安装nvidia-tensorflow</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> --user nvidia-tensorflow</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">horovod</span><span class="token punctuation" style="color:#393A34">]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>​			输入命令行之后静静等待即可。</p><p>​			如果是无限下载，也就是完整下载了某个依赖包之后又重新下载这一个依赖包，同时伴有pip报错。解决方案就是将该环境删除，重新配置。但是出现下面的情况是完好的，不用紧张，静静等待即可：			<img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729101519195.png!blog.luomoe.com.20230729" alt="image-20230729101519195" class="img_ev3q"></p><p>​	（4）验证安装。待上述的步骤安装结束之后，即可验证TF是否安装在本地。首先是保证已经进入了已经安装的TF 1.x的环境，之后操作图下：</p><p>​			进入python环境：</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>​			引入TF:  </p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> tensorflow </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> tf</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>​			验证：</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">"Num GPUs Available: "</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token builtin">len</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">tf</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">config</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">experimental</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">list_physical_devices</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">'GPU'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">print</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">"Num GPUs Available: "</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token builtin">len</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">tf</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">config</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">experimental</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">list_physical_devices</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">'XLA_GPU'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>​		(5) 如果出现了如下情况就表示已经安装成功了：</p><p>​				<strong>3090显卡：</strong></p><p><img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729102856243.png!blog.luomoe.com.20230729" alt="image-20230729102856243" class="img_ev3q"></p><p>​			</p><p>​			<strong>4090显卡：</strong></p><p><img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729102946121.png!blog.luomoe.com.20230729" alt="image-20230729102946121" class="img_ev3q"></p><p>​	</p><p>​	能够识别出服务器显卡的的参数即表示TF 1.x版本的安装成功</p></li></ol><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="实验试跑">实验试跑<a class="hash-link" href="#实验试跑" title="标题的直接链接">​</a></h2><p>安装成功了，怎么能不试着跑一下呢。安装部署了自己所需的TF环境，在按照需求安装其他的依赖包，即可试着跑自己的代码。</p><p>此处，本文使用了某平台上的两个GPU服务器跑代码，配置如下：</p><p><img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729103439357.png!blog.luomoe.com.20230729" alt="image-20230729103439357" class="img_ev3q"></p><p>之后，部署其他的依赖包，直接上传代码跑实验：</p><p><img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729103811417.png!blog.luomoe.com.20230729" alt="image-20230729103811417" class="img_ev3q"></p><p>总体效果还不错。</p><p>​			3090卡：		<img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729103922165.png!blog.luomoe.com.20230729" alt="image-20230729103922165" class="img_ev3q"></p><p>​			</p><p>​			4090卡：</p><p><img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/assets/image-20230729104011556.png!blog.luomoe.com.20230729" alt="image-20230729104011556" class="img_ev3q"></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="结语">结语<a class="hash-link" href="#结语" title="标题的直接链接">​</a></h2><p>挺好，不用重写代码了。</p><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>说明</div><div class="admonitionContent_S0QG"><p>我知道有机器采集网站文章，所以已经在公众号上先发了。</p><p><img loading="lazy" src="https://img.up.cdn.nahida.cn/typora/20230729/wechat.png!blog.luomoe.com.20230729" alt="image-20230729093126683" class="img_ev3q"></p></div></div>]]></content>
        <author>
            <name>Pangluo</name>
            <uri>https://luomoe.com</uri>
        </author>
        <category label="tensorflow" term="tensorflow"/>
        <category label="NVIDIA" term="NVIDIA"/>
        <category label="RTX4090" term="RTX4090"/>
        <category label="RTX3090" term="RTX3090"/>
        <category label="AI" term="AI"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[边缘计算相关论文笔记（2023年6月）]]></title>
        <id>IoV202306</id>
        <link href="https://blog.luomoe.com/blog/IoV202306"/>
        <updated>2023-06-30T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[001  Cooperative Dynamic Voltage Scaling and Radio Resource Allocation for Energy-Efficient Multiuser Mobile Edge Computing]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="001--cooperative-dynamic-voltage-scaling-and-radio-resource-allocation-for-energy-efficient-multiuser-mobile-edge-computing">001  Cooperative Dynamic Voltage Scaling and Radio Resource Allocation for Energy-Efficient Multiuser Mobile Edge Computing<a class="hash-link" href="#001--cooperative-dynamic-voltage-scaling-and-radio-resource-allocation-for-energy-efficient-multiuser-mobile-edge-computing" title="标题的直接链接">​</a></h2><p>This article investigates cooperative dynamic voltage regulation and wireless resource allocation in multi-user mobile edge computing for energy efficient computation offloading. The article proposes a suboptimal algorithm based on Lagrangian pairwise decomposition to minimize the weighted sum of mobile energy consumption by jointly optimizing the computational speed of smart mobile devices, subcarrier allocation, transmit power of each subcarrier, data size sent per subcarrier, and offloading ratio. Simulation results show that the algorithm converges quickly and can significantly reduce energy consumption. In addition, the paper finds that the total mobile energy consumption remains stable or increases with the variance of the delay requirement for a given delay mean, which can guide the access control in practice.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="002--energy-efficient-resource-allocation-for-mobile-edge-computation-offloading">002  Energy-Efficient Resource Allocation for Mobile-Edge Computation Offloading<a class="hash-link" href="#002--energy-efficient-resource-allocation-for-mobile-edge-computation-offloading" title="标题的直接链接">​</a></h2><p>The article investigates the resource allocation of multi-user mobile edge computing offload (MECO) systems based on time division multiple access (TDMA) and orthogonal frequency division multiple access (OFDMA). The article first investigates TDMA MECO systems with infinite or finite cloud computing capabilities and formulates the optimal resource allocation problem as a convex optimization problem. Then, the authors consider OFDMA MECO systems and formulate the optimal resource allocation problem as a mixed integer problem. By converting the OFDMA problem into its TDMA counterpart, the authors propose a low-complexity suboptimal algorithm and show near-optimal performance in simulations. In summary, this paper investigates the energy efficient resource allocation problem for mobile edge computing offload and proposes corresponding optimal and suboptimal algorithms.</p><h1>第二周</h1><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="003-game-and-contract-theory-based-energy-transaction-management-for-internet-of-electric-vehicle">003 Game and Contract Theory-Based Energy Transaction Management for Internet of Electric Vehicle<a class="hash-link" href="#003-game-and-contract-theory-based-energy-transaction-management-for-internet-of-electric-vehicle" title="标题的直接链接">​</a></h2><p>This is a research paper on game and contract theory based power transaction management in smart grid systems. The article proposes a three-tier bi-directional electric energy trading management strategy, including an energy grid as an energy supplier, an energy aggregator as an energy distributor, and an electric vehicle as an energy provider. The article uses a Stackelberg game to solve the optimal pricing and electric vehicle discharging problems, and proposes an incentive mechanism based on contract theory to motivate electric vehicles to participate in energy trading and optimize the utility of energy aggregators. Simulation results show that the proposed scheme performs significantly better than other existing schemes in various scenarios.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="004--joint-offloading-and-computing-optimization-in-wireless-powered-mobile-edge-computing-systems">004  Joint Offloading and Computing Optimization in Wireless Powered Mobile-Edge Computing Systems<a class="hash-link" href="#004--joint-offloading-and-computing-optimization-in-wireless-powered-mobile-edge-computing-systems" title="标题的直接链接">​</a></h2><p>In this paper, the study the problem of joint offloading and computational optimization in wireless mobile edge computing systems. The paper considers a wirelessly powered multi-user MEC system consisting of multiple antenna access points (APs) and multiple users, where the APs (integrated MEC servers) transmit energy via radio waves to charge multiple users, and each user node relies on the collected energy to perform latency-sensitive computational tasks. Through the MEC, these users can perform their respective tasks locally by themselves or offload all or part of them to the AP according to the time division multiple access (TDMA) protocol.In this case, this study optimizes the MEC-WPT system by jointly optimizing the transmitted energy beamformer of the AP, the central processing unit (CPU) frequency and offload bits of each user, and the time allocation between different users design to pursue energy efficiency. Specifically, this study minimizes the energy consumption of the access point in a given time block under the computational delay and energy harvesting constraints for each user. By transforming this problem into a convex framework and using the Lagrangian dual method, an optimal solution in semi-closed form is obtained in this paper. Numerical results show that the proposed joint design outperforms other benchmark solutions in terms of achieving energy efficiency.</p><h1>第三周</h1><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="005-learning-based-energy-efficient-task-offloading-for-vehicular-collaborative-edge-computing">005 Learning Based Energy Efficient Task Offloading for Vehicular Collaborative Edge Computing<a class="hash-link" href="#005-learning-based-energy-efficient-task-offloading-for-vehicular-collaborative-edge-computing" title="标题的直接链接">​</a></h2><p>This paper is a paper on Vehicular Collaborative Edge Computing (VCEC). The article proposes an energy efficient task offloading approach based on learning, which aims to reduce energy consumption within the VCEC system by maximizing the use of idle and redundant resources of vehicles. The authors apply Lyapunov optimization to decompose the original problem into three subproblems and solve them one by one by addressing the challenges of short-term decision and long-term queueing delay constraints, information uncertainty, and task offloading conflicts. These three subproblems are 1) short-term task unloading decision, 2) long-term queueing delay constraint, and 3) information uncertainty and task unloading conflict. The results of extensive numerical simulations show that the method outperforms the benchmark method in terms of energy consumption, learning regret, task backlog and end-to-end delay.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="006---mobile-edge-computing-enabled-internet-of-vehicles-toward-energy-efficient-scheduling">006   Mobile Edge Computing-Enabled Internet of Vehicles: Toward Energy-Efficient Scheduling<a class="hash-link" href="#006---mobile-edge-computing-enabled-internet-of-vehicles-toward-energy-efficient-scheduling" title="标题的直接链接">​</a></h2><p>This article discusses the construction of green cities in modern transportation systems. The article points out that although modern transportation systems facilitate the daily life of citizens, increasing energy consumption and air pollution pose challenges to the construction of green cities. Currently, research on green IoVs has focused on battery-backed RSUs or energy management of electric vehicles. However, the computational tasks and load balancing among RSUs have not been fully studied. To meet the heterogeneous requirements of communication, computation and storage in IoV, this paper constructs an energy-efficient scheduling framework for minimizing the energy consumption of RSUs in MEC-supported IoV. Specifically, the paper proposes a heuristic algorithm to achieve this by jointly considering task scheduling among MEC servers and the downlink energy consumption of RSUs. To the best of our knowledge, this is the first work to focus on the problem of energy consumption control of MEC-enabled RSUs. The performance evaluation shows that the framework is effective in terms of energy consumption, latency and task blocking possibilities. Finally, the paper details some of the main challenges and open issues and identifies future research directions including renewable energy recharge, sustainable and reliable MEC, incentive and trusted offloading, and deep learning-based scheduling.</p><h1>第四周</h1><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="007-multi-user-multi-task-computation-offloading-in-green-mobile-edge-cloud-computing">007 Multi-User Multi-Task Computation Offloading in Green Mobile Edge Cloud Computing<a class="hash-link" href="#007-multi-user-multi-task-computation-offloading-in-green-mobile-edge-cloud-computing" title="标题的直接链接">​</a></h2><p>This article discusses multi-user multi-task computation offloading in green mobile edge cloud computing. The article proposes a multi-user multi-task computation offloading framework that takes into account the dynamics of energy in the mobile edge cloud and the dynamics of tasks in different mobile devices. The article also proposes a centralized and distributed greedy maximum scheduling algorithm and discusses the performance bounds of the proposed scheme. Simulation results show that the proposed scheduling algorithm provides an average system utility improvement of 18.8% to 31.9% over the random scheduling scheme.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="008-weighted-energy-efficiency-maximization-for-a-uav-assisted-multiplatoon-mobile-edge-computing-system">008 Weighted Energy-Efficiency Maximization for a UAV-Assisted Multiplatoon Mobile-Edge Computing System<a class="hash-link" href="#008-weighted-energy-efficiency-maximization-for-a-uav-assisted-multiplatoon-mobile-edge-computing-system" title="标题的直接链接">​</a></h2><p>This article investigates a UAV-assisted multi-fleet mobile edge computing system that aims to maximize the weighted global energy efficiency of the system. The article designs a fleet controller based on a 2-D path-tracking model and the Frenet framework, and simulates the coupling characteristics of air-to-ground communication and on-board computing. Due to the non-convexity of the objective function and constraints of the optimization problem, the article proposes an optimization algorithm based on sequential quadratic programming (SQP) method. Simulation results show that the proposed method significantly outperforms the conventional scheme. This paper provides new ideas and methods to improve the energy efficiency of the system by studying the UAV-assisted multi-fleet mobile edge computing system.</p><h1>第五周</h1><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="009--fast-adaptive-task-offloading-in-edge-computing-based-on-meta-reinforcement-learning">009  Fast Adaptive Task Offloading in Edge Computing Based on Meta Reinforcement Learning<a class="hash-link" href="#009--fast-adaptive-task-offloading-in-edge-computing-based-on-meta-reinforcement-learning" title="标题的直接链接">​</a></h2><p>This article presents a fast adaptive task offloading method based on meta-reinforcement learning for solving the task offloading problem in multi-access edge computing (MEC). The article first introduces the background of MEC and the challenges of the task offloading problem. Then, the article proposes a meta-reinforcement learning-based task offloading method that can quickly adapt to new environments with a small number of gradient updates and samples. The article models the mobile application as a directed acyclic graph (DAG) and uses a custom sequence-to-sequence (seq2seq) neural network to model the offloading strategy. To effectively train the seq2seq network, the article proposes a method that combines first-order approximations and truncated alternative targets. Experimental results show that the method is able to reduce the latency by up to 25% compared to three benchmark algorithms, while being able to quickly adapt to new environments. In conclusion, this article proposes a fast adaptive task offloading method based on meta-reinforcement learning that can effectively solve the task offloading problem in MEC.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="010-meta-reinforcement-learning-for-multi-task-offloading-in-vehicular-edge-computing">010 Meta Reinforcement Learning for Multi-task Offloading in Vehicular Edge Computing<a class="hash-link" href="#010-meta-reinforcement-learning-for-multi-task-offloading-in-vehicular-edge-computing" title="标题的直接链接">​</a></h2><p>This paper investigates the problem of multitask offloading in vehicular edge computing. Due to the highly dynamic nature of the vehicle environment and the heterogeneous characteristics of vehicle services, traditional expert-based or learning-based strategies require updating manual parameters or retraining learning models, which leads to intolerable overhead. Therefore, in this paper, a Seq2seq-based meta-reinforcement learning algorithm is proposed for solving the multitask offloading problem. Specifically, a bidirectional gated cyclic unit integrated attention mechanism is designed to determine unloading actions by encoding sequential unloading actions and displaying different preferences for input sequences. In particular, a meta-reinforcement learning framework is designed based on a model agnostic meta-learning framework that trains meta-strategies offline and quickly adapts to new multitask offloading scenarios within a few training steps. Finally, this paper evaluates the performance based on the task generator DAGGEN and real vehicle trajectories, and the results show that SMRL-MTO reduces the task execution time by 11.36% on average compared to the greedy algorithm.</p>]]></content>
        <author>
            <name>Pangluo</name>
            <uri>https://luomoe.com</uri>
        </author>
        <category label="Edge Computing" term="Edge Computing"/>
        <category label="Offloading" term="Offloading"/>
        <category label="Vehicles" term="Vehicles"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[边缘计算相关论文笔记（2023年5月）]]></title>
        <id>IoV202305</id>
        <link href="https://blog.luomoe.com/blog/IoV202305"/>
        <updated>2023-05-31T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[001  A coordinated control to improve performance for a building cluster with energy storage, electric vehicles, and energy sharing considered]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="001--a-coordinated-control-to-improve-performance-for-a-building-cluster-with-energy-storage-electric-vehicles-and-energy-sharing-considered">001  A coordinated control to improve performance for a building cluster with energy storage, electric vehicles, and energy sharing considered<a class="hash-link" href="#001--a-coordinated-control-to-improve-performance-for-a-building-cluster-with-energy-storage-electric-vehicles-and-energy-sharing-considered" title="标题的直接链接">​</a></h2><p><a href="https://doi.org/10.1016/j.apenergy.2020.114983" target="_blank" rel="noopener noreferrer">https://doi.org/10.1016/j.apenergy.2020.114983</a></p><p>This article is about a study that proposes a coordinated control approach for optimizing the performance of building complexes with energy storage, electric vehicles and energy sharing.This study proposes a coordinated control approach to optimize the performance of a building complex with energy storage, electric vehicles, and energy sharing. The study first develops an electric vehicle charging and discharging model, then based on predicted electricity demand and renewable energy generation data for the next 24 hours, the coordinated control first considers the entire complex as an "integrated" building and uses a genetic algorithm to optimize its operation and the charging and discharging of electric vehicles. Next, non-linear planning is used to coordinate the operation of each building over the next 24 hours. For validation, the developed control has been tested on a real building complex in Ludvika, Sweden. The results of the study show that the developed control can increase the daily renewable self-use rate at the cluster level by 19% compared to the conventional control, while reducing the daily electricity bill by 36%.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="002--an-improved-soc-control-strategy-for-electric-vehicle-hybrid-energy-storage-systems">002  An Improved SOC Control Strategy for Electric Vehicle Hybrid Energy Storage Systems<a class="hash-link" href="#002--an-improved-soc-control-strategy-for-electric-vehicle-hybrid-energy-storage-systems" title="标题的直接链接">​</a></h2><p><a href="https://doi.org/10.3390/en13205297" target="_blank" rel="noopener noreferrer">https://doi.org/10.3390/en13205297</a></p><p>This is an article on hybrid energy storage system for electric vehicles. The article proposes an optimized power distribution method using two isolated soft-switched symmetrical half-bridge bidirectional converters connecting a battery and a supercapacitor as a composite structure for the protection structure. The article mentions that hybrid energy storage system (HESS) is an effective method to improve the performance of electric vehicles and extend the battery life. In such systems, batteries and supercapacitors are connected in parallel to provide higher peak power and better energy management. The article proposes a novel HESS structure in which two isolated soft-switched symmetrical half-bridge bidirectional converters are used to connect the battery and the supercapacitor. This structure provides better protection and can be optimized for energy management through an improved energy distribution strategy based on SOC control. This strategy allows the supercapacitor to be charged and discharged at a peak current of about 4ibat and can be adapted to different types of load profiles. Experimental results show that the use of this HESS structure and energy allocation strategy can improve the acceleration performance of electric vehicles by about 50% and reduce energy losses by about 69% compared to the battery-only mode. This approach not only improves energy utilization, but also reduces battery aging effects.</p><h1>第二周</h1><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="003-adaptive-de-algorithm-for-novel-energy-control-framework-based-on-edge-computing-in-iiot-applications">003 Adaptive DE Algorithm for Novel Energy Control Framework Based on Edge Computing in IIoT Applications<a class="hash-link" href="#003-adaptive-de-algorithm-for-novel-energy-control-framework-based-on-edge-computing-in-iiot-applications" title="标题的直接链接">​</a></h2><blockquote><p>DOI 10.1109/TII.2020.3007644</p></blockquote><p>This is a paper on a novel energy control framework based on edge computing for industrial IoT applications. The paper proposes an efficient energy control framework to reduce energy waste and increase benefits for industrial users through edge computing. For this purpose, battery storage systems are used to store energy to ensure supply stability and power quality. With this framework, the optimal load pattern and the corresponding storage capacity of the battery storage system can be obtained based on historical load data from energy markets and industrial users. However, calculating these requires consideration of trade-offs between equipment costs, time-of-use tariffs, operating costs, and other relevant factors, which would be an NP-hard problem. To address this challenge, the authors also propose an adaptive hybrid differential evolutionary algorithm with a novel variational strategy. The experimental results show that the proposed algorithm and framework have good results.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="004--secure-and-efficient-vehicle-to-grid-energy-trading-in-cyber-physical-systems-integration-of-blockchain-and-edge-computing">004  Secure and Efficient Vehicle-to-Grid Energy Trading in Cyber Physical Systems: Integration of Blockchain and Edge Computing<a class="hash-link" href="#004--secure-and-efficient-vehicle-to-grid-energy-trading-in-cyber-physical-systems-integration-of-blockchain-and-edge-computing" title="标题的直接链接">​</a></h2><p>This is an academic paper on vehicle-to-grid (V2G) energy trading. The article proposes a secure and efficient framework for V2G energy transactions by integrating blockchain, contract theory, and edge computing. First, the article develops a secure energy trading mechanism based on a federated blockchain. Then, an efficient incentive mechanism based on contract theory is proposed considering the information asymmetry. Next, edge computing is introduced to improve the success probability of block creation. Finally, the performance of the proposed framework is verified by numerical results and theoretical analysis. In summary, this paper investigates how to integrate blockchain, contract theory, and edge computing to achieve security and efficiency in V2G energy transactions.</p><h1>第三周</h1><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="005-energy-efficient-task-caching-and-offloading-for-mobile-edge-computing">005 Energy Efficient Task Caching and Offloading for Mobile Edge Computing<a class="hash-link" href="#005-energy-efficient-task-caching-and-offloading-for-mobile-edge-computing" title="标题的直接链接">​</a></h2><p>This paper investigates task caching and offloading in mobile edge computing. The authors first present task caching on edge clouds, which is the first study of task caching in mobile edge computing. The article further investigates task caching and offloading policies that determine which tasks should be cached and how many tasks should be offloaded. The goal is to minimize the total energy consumption of mobile devices while satisfying user latency requirements. The authors formulate this problem as a mixed-integer nonlinear programming problem and propose an efficient algorithm to solve it. Simulation results show that the proposed scheme in this paper has a lower energy cost compared to other schemes. Future work will consider multiple edge cloud task caching and offloading strategies. The experimental results show that the energy cost of mobile devices can be effectively reduced by rational deployment of cache location and task offloading.</p>]]></content>
        <author>
            <name>Pangluo</name>
            <uri>https://luomoe.com</uri>
        </author>
        <category label="Edge Computing" term="Edge Computing"/>
        <category label="Offloading" term="Offloading"/>
        <category label="Vehicles" term="Vehicles"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[友情链接]]></title>
        <id>/links</id>
        <link href="https://blog.luomoe.com/blog/links"/>
        <updated>2023-03-26T02:46:30.000Z</updated>
        <summary type="html"><![CDATA[个人其他站点: 世界第一可爱]]></summary>
        <content type="html"><![CDATA[<h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="个人其他站点-世界第一可爱">个人其他站点: <a href="https://www.nahida.cn/" target="_blank" rel="noopener noreferrer">世界第一可爱</a><a class="hash-link" href="#个人其他站点-世界第一可爱" title="标题的直接链接">​</a></h3><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>信息</div><div class="admonitionContent_S0QG"><p>（交换友链请点击👉<a href="mailto:i@lyyousa.com" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="http://rescdn.qqmail.com/zh_CN/htmledition/images/function/qm_open/ico_mailme_01.png" alt="img" class="img_ev3q"></a> ）</p><p>本站的名称：胖螺</p><p>说明：无限进步</p><p>本站站点地址： <a href="https://blog.luomoe.com/" target="_blank" rel="noopener noreferrer">https://blog.luomoe.com/</a></p><p>本人头像： <a href="https://img.up.cdn.nahida.cn/2020/03/cropped-logo2-1.png" target="_blank" rel="noopener noreferrer">https://img.up.cdn.nahida.cn/2020/03/cropped-logo2-1.png</a></p><p>要求：站子有原创文章，无版权争议，有https信仰加持，在您那里也能看到胖螺的身影，不接受单向友链。交换直接评论留言即可~</p><p>后续：博主会不定期检查友链的情况，如果出现了网站无法访问、上述要求没有满足、遇盗版、破解、网页植入挖矿脚本、极低质量内容、单方面移除本站链接的。胖螺也会单方面移除链接哦，见谅~</p></div></div>]]></content>
        <author>
            <name>Luminous' Home</name>
            <uri>https://luotianyi.vc/</uri>
        </author>
        <author>
            <name>洛水.山岭居室</name>
            <uri>https://luoshuijs.vip/</uri>
        </author>
        <author>
            <name>CZM有点冷</name>
            <uri>https://czm.cool/</uri>
        </author>
        <author>
            <name>兀云资源网</name>
            <uri>https://www.itliujia.cn/</uri>
        </author>
        <author>
            <name>抵到烂</name>
            <uri>https://www.didaolan.cn/</uri>
        </author>
        <author>
            <name>Mlikiowa Home Village</name>
            <uri>https://nanaeo.cn/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[欢迎]]></title>
        <id>welcome</id>
        <link href="https://blog.luomoe.com/blog/welcome"/>
        <updated>2023-01-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[在重邮实习，第一次到重庆，感受真的不一样]]></summary>
        <content type="html"><![CDATA[<p><strong>在重邮实习，第一次到重庆，感受真的不一样</strong></p><p>感觉还是在学校里舒坦啊~</p><p>——2023年7月25日</p><p><strong>备案了个新域名，名字居然给过了</strong></p><p>换了个省份备案。原本只是尝试一下，没想到真的过了。不愧是四川管局。要是换以前，名字卡的要死，最后名字变得扭扭捏捏奇奇怪怪的。</p><p>——2023年3月25日</p><p><strong>再一次优化和整理自己的博客，把一些博客文章整理为笔记。</strong></p><p>疫情真的太不容易啦，开始开源节流，停用各种自租云服务器，拥抱各种Page。</p><p>——2023年1月9日</p><p>太卷了太卷了，转全栈。
今夜天星璀璨，月相壮丽，想必是个好日子。嘶，待我一算...嗯~我知道我知道，是你的生辰!哎呀，真是个好日子。</p><p>——2022年7月15日。胡桃生日（钓鱼不成，自己跳到水里抓鱼，真的太可爱了，）</p><p>再次归来，对网站内容进行了重新优化，减去过多且无用的分类。删去其他花里胡哨的东西，在一定程度上降低二次元浓度。</p><p>将只专注于前端相关，记录自己踩过的坑，应该也会有其他的东西（非计算机的），但是不会太多。方便未来的自己的在前端踩坑时候再找回相对应的步骤。目前只要求自己能看懂。如果有小伙伴想了解相关的内容但是又看不懂，请再对应的内容评论，我看到后会第一时间修改和完善有关的描述。</p><p>——2022年1月31日，农历牛年除夕夜。</p>]]></content>
        <author>
            <name>Pangluo</name>
            <uri>https://luomoe.com</uri>
        </author>
    </entry>
</feed>